{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1. Install PySpark, Java, Hadoop (required to run PySpark)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ZiweiMengyang/Desktop/Python & Machine Learning/Tiger/Capstone/Codes'"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is the Python API for Spark\n",
    "# SparkContext is a public class in PySpark. It is the main entry point for Spark functionality in Python\n",
    "# .getOrCreate() is a classmethod that instantiate a SparkContext\n",
    "\n",
    "from pyspark import SparkContext \n",
    "sc = SparkContext.getOrCreate()\n",
    "# i.e. sc is created as a SparkContext class\n",
    "\n",
    "# create RDD using SparkContext\n",
    "# textFile() reads a text from HDFS, a local file system, or Hadoop-supported file system,\n",
    "# and return it as an RDD of strings\n",
    "\n",
    "rdd = sc.textFile(name=\"../Data/all_play_log.log.fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['154422682 \\tar \\t20870993 \\t1 \\t用情 \\t狮子合唱团 \\t22013 \\t332 \\t0 \\t 20170301_play.log',\n",
       " '154421907 \\tip \\t6560858 \\t0 \\t表情不要悲伤 \\t伯贤&D.O.&张艺兴&朴灿烈 \\t96 \\t161 \\t0 \\t 20170301_play.log']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "MEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Next, create pyspark.sql.SparkSession, which is the main entry point for DataFrame and SQL functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Parse log file into RDD then into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MusicBox Capstone\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# appName: Sets a name for the application, which will be shown in the Spark web UI\n",
    "# config(string for configuration property, value string)\n",
    "# config(conf=SparkConf())\n",
    "# can create a class pyspark.SparkConf(..), and call it in when building a SparkSession\n",
    "# .config(\"spark.some.config.option\", \"some-value\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 4 minutes to run.... cautious\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a sample log, get a sense how to create a schema for parsing log file into DataFrame\n",
    "# schema_orig = ['uid','device','song_id','song_type','song_name','singer','play_time','song_length','paid_flag']\n",
    "# df = pd.read_csv('../Data/Play/20170331_1_play.log',delimiter='\\t',header=None,index_col=None,names=schema)\n",
    "# df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(\"\\t\")\n",
    "    if len(fields) == 10:\n",
    "        try: \n",
    "            uid = str(fields[0])\n",
    "            device = str(fields[1])\n",
    "            song_id = float(fields[2])\n",
    "            song_type = float(fields[3])\n",
    "            song_name = str(fields[4])\n",
    "            singer = str(fields[5])\n",
    "            play_time = float(fields[6])\n",
    "            song_length = float(fields[7])\n",
    "            paid_flag = float(fields[8])\n",
    "            file_name = str(fields[9])\n",
    "            return Row(uid, device, song_id, song_type, song_name, singer, play_time, song_length, paid_flag, file_name)\n",
    "        except:\n",
    "            return -1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Create Row entries that specify column name, to prepare the RDD to convert it to a DataFrame\n",
    "# Always important to filter on field length after splitting, to avoid \"index out of range error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide schema in order to create DataFrame\n",
    "# Spark SQL StructType is the data type representing rows. \n",
    "# A StructType object comprises a list of StructField, which represents a field in a StructType\n",
    "# StructField(name of this field, dataType, nullable)\n",
    "\n",
    "\n",
    "schema = StructType([StructField('uid', StringType(), False),\n",
    "                     StructField('device', StringType(), True),\n",
    "                     StructField('song_id', FloatType(), False),\n",
    "                     StructField('song_type', FloatType(), True),\n",
    "                     StructField('song_name', StringType(), True),\n",
    "                     StructField('singer', StringType(), True),\n",
    "                     StructField('play_time', FloatType(), False),\n",
    "                     StructField('song_length', FloatType(), True),\n",
    "                     StructField('paid_flag', FloatType(), True),\n",
    "                     StructField('file_name', StringType(), True),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = rdd.map(parseLine).filter(lambda x: x!= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = spark.createDataFrame(songs, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(uid,StringType,false),StructField(device,StringType,true),StructField(song_id,FloatType,false),StructField(song_type,FloatType,true),StructField(song_name,StringType,true),StructField(singer,StringType,true),StructField(play_time,FloatType,false),StructField(song_length,FloatType,true),StructField(paid_flag,FloatType,true),StructField(file_name,StringType,true)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>device</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_type</th>\n",
       "      <th>song_name</th>\n",
       "      <th>singer</th>\n",
       "      <th>play_time</th>\n",
       "      <th>song_length</th>\n",
       "      <th>paid_flag</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154422682</td>\n",
       "      <td>ar</td>\n",
       "      <td>20870992.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>用情</td>\n",
       "      <td>狮子合唱团</td>\n",
       "      <td>22013.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154421907</td>\n",
       "      <td>ip</td>\n",
       "      <td>6560858.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>表情不要悲伤</td>\n",
       "      <td>伯贤&amp;D.O.&amp;张艺兴&amp;朴灿烈</td>\n",
       "      <td>96.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154422630</td>\n",
       "      <td>ar</td>\n",
       "      <td>3385963.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Baby, Don't Cry(人鱼的眼泪)</td>\n",
       "      <td>EXO</td>\n",
       "      <td>235868.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154410267</td>\n",
       "      <td>ar</td>\n",
       "      <td>6777172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3D-环绕音律1(3D Mix)</td>\n",
       "      <td>McTaiM</td>\n",
       "      <td>164.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154407793</td>\n",
       "      <td>ar</td>\n",
       "      <td>19472464.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>刚好遇见你</td>\n",
       "      <td>曲肖冰</td>\n",
       "      <td>24.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid device     song_id  song_type                song_name  \\\n",
       "0  154422682     ar   20870992.0        1.0                      用情    \n",
       "1  154421907     ip    6560858.0        0.0                  表情不要悲伤    \n",
       "2  154422630     ar    3385963.0        1.0  Baby, Don't Cry(人鱼的眼泪)    \n",
       "3  154410267     ar    6777172.0        0.0        3D-环绕音律1(3D Mix)    \n",
       "4  154407793     ar   19472464.0        0.0                   刚好遇见你    \n",
       "\n",
       "             singer  play_time  song_length  paid_flag           file_name  \n",
       "0            狮子合唱团     22013.0        332.0        0.0   20170301_play.log  \n",
       "1  伯贤&D.O.&张艺兴&朴灿烈        96.0        161.0        0.0   20170301_play.log  \n",
       "2              EXO    235868.0        235.0        0.0   20170301_play.log  \n",
       "3           McTaiM       164.0        237.0        0.0   20170301_play.log  \n",
       "4              曲肖冰        24.0        201.0        0.0   20170301_play.log  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(song_df.take(5), columns=song_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Sanity Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.groupBy('uid').count().orderBy('count', ascending = False).show(truncate=False)\n",
    "# takes hours to run... DO NOT re-run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding 1: Obviously some uid are testing accounts (i.e. robots) and should be excluded from the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.select('play_time', 'song_length', 'paid_flag').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df_2 = song_df.withColumn(\"device\", trim(song_df.device)) \\\n",
    "                               .withColumn('date_str', trim(song_df.file_name.substr(1,9))) \\\n",
    "                               .withColumn('date_string', regexp_replace('date_str', '20170339', '20170329')) \\\n",
    "                               .withColumn(\"unix_date\", unix_timestamp('date_string', 'yyyyMMdd')) \\\n",
    "                               .withColumn(\"date\", from_unixtime('unix_date').cast(DateType())) \\\n",
    "                               .drop('date_str') \\\n",
    "                               .drop('date_string') \\\n",
    "                               .drop('unix_date') \\\n",
    "                               .drop('paid_flag') \\\n",
    "                               .drop('song_name') \\\n",
    "                               .drop('singer') \\\n",
    "                               .dropna(how='any', subset=['play_time']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>device</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_type</th>\n",
       "      <th>play_time</th>\n",
       "      <th>song_length</th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154422682</td>\n",
       "      <td>ar</td>\n",
       "      <td>20870992.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22013.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154421907</td>\n",
       "      <td>ip</td>\n",
       "      <td>6560858.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154422630</td>\n",
       "      <td>ar</td>\n",
       "      <td>3385963.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>235868.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154410267</td>\n",
       "      <td>ar</td>\n",
       "      <td>6777172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154407793</td>\n",
       "      <td>ar</td>\n",
       "      <td>19472464.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid device     song_id  song_type  play_time  song_length  \\\n",
       "0  154422682      ar  20870992.0        1.0    22013.0        332.0   \n",
       "1  154421907      ip   6560858.0        0.0       96.0        161.0   \n",
       "2  154422630      ar   3385963.0        1.0   235868.0        235.0   \n",
       "3  154410267      ar   6777172.0        0.0      164.0        237.0   \n",
       "4  154407793      ar  19472464.0        0.0       24.0        201.0   \n",
       "\n",
       "            file_name        date  \n",
       "0   20170301_play.log  2017-03-01  \n",
       "1   20170301_play.log  2017-03-01  \n",
       "2   20170301_play.log  2017-03-01  \n",
       "3   20170301_play.log  2017-03-01  \n",
       "4   20170301_play.log  2017-03-01  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(song_df_2.take(5), columns=song_df_2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding 2: I noticed play log files are missing from 20170310 to 20170328, download log and search log start from 20170330, and therefore I only use play logs from 20170330\n",
    "### Finding 3: play_time and song_length may be recorded in milli-seconds as opposed to seconds in some records. Judging from limited records, play_time in milli-seconds seem to relate to song_time = 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df_3 = song_df_2.drop('file_name') \\\n",
    "                     .filter(song_df_2.date>='2017-03-30') \\\n",
    "                     .filter(song_df_2.song_length > 0) \\\n",
    "                     .filter(song_df_2.song_length < 10E6) \\\n",
    "                     .filter(song_df_2.play_time >= 0) \\\n",
    "                     .filter(song_df_2.play_time<10E6)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>device</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_type</th>\n",
       "      <th>play_time</th>\n",
       "      <th>song_length</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168550892</td>\n",
       "      <td>ar</td>\n",
       "      <td>23491656.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>2017-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168540455</td>\n",
       "      <td>ar</td>\n",
       "      <td>298250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2017-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168551247</td>\n",
       "      <td>ar</td>\n",
       "      <td>11881432.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>2017-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168549788</td>\n",
       "      <td>ip</td>\n",
       "      <td>295469.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>2017-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168551248</td>\n",
       "      <td>ip</td>\n",
       "      <td>21393368.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2017-03-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         uid device     song_id  song_type  play_time  song_length        date\n",
       "0  168550892     ar  23491656.0        0.0      254.0        254.0  2017-03-30\n",
       "1  168540455     ar    298250.0        0.0      189.0        190.0  2017-03-30\n",
       "2  168551247     ar  11881432.0        0.0       78.0        149.0  2017-03-30\n",
       "3  168549788     ip    295469.0        0.0       16.0        242.0  2017-03-30\n",
       "4  168551248     ip  21393368.0        0.0       87.0         87.0  2017-03-30"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(song_df_3.take(5), columns=song_df_3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test my hypothesis that song_type is related to play_time recorded in milli-seconds\n",
    "song_df_3.sample(False, 0.00001, seed=0) \\\n",
    "         .groupBy('song_type') \\\n",
    "         .agg({'play_time': 'avg', \\\n",
    "               'song_length':'avg' }) \\\n",
    "         .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df_3.sample(False, 0.00001, seed=0).groupBy('song_type').agg({'play_time': \"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df_4 = song_df_3.withColumn('play_time_adj', \\\n",
    "                                 when(song_df_3.play_time > 10E5, song_df_3.play_time/1000) \\\n",
    "                                 .otherwise(song_df_3.play_time)) \\\n",
    "                     .withColumn('song_length_adj', \\\n",
    "                                 when(song_df_3.song_length > 10E4, song_df_3.song_length/1000) \\\n",
    "                                 .otherwise(song_df_3.song_length))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df_4.sample(False, 0.00001, seed=0).groupBy('song_type').agg({'play_time': \"max\", \"uid\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ! ! song_df_5 is the data with cleaned up \"play_time\" and \"song_length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df_5 = song_df_4.drop('play_time') \\\n",
    "                     .drop('song_length') \\\n",
    "                     .withColumnRenamed('play_time_adj', 'play_time') \\\n",
    "                     .withColumnRenamed('song_length_adj', 'song_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, device: string, song_id: float, song_type: float, date: date, play_time: double, song_length: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_df_5.persist(MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(uid,StringType,false),StructField(device,StringType,true),StructField(song_id,FloatType,false),StructField(song_type,FloatType,true),StructField(date,DateType,true),StructField(play_time,DoubleType,true),StructField(song_length,DoubleType,true)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_df_5.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>device</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_type</th>\n",
       "      <th>date</th>\n",
       "      <th>play_time</th>\n",
       "      <th>song_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168550892</td>\n",
       "      <td>ar</td>\n",
       "      <td>23491656.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-30</td>\n",
       "      <td>254.0</td>\n",
       "      <td>254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168540455</td>\n",
       "      <td>ar</td>\n",
       "      <td>298250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-30</td>\n",
       "      <td>189.0</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168551247</td>\n",
       "      <td>ar</td>\n",
       "      <td>11881432.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-30</td>\n",
       "      <td>78.0</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168549788</td>\n",
       "      <td>ip</td>\n",
       "      <td>295469.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-30</td>\n",
       "      <td>16.0</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168551248</td>\n",
       "      <td>ip</td>\n",
       "      <td>21393368.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-30</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         uid device     song_id  song_type        date  play_time  song_length\n",
       "0  168550892     ar  23491656.0        0.0  2017-03-30      254.0        254.0\n",
       "1  168540455     ar    298250.0        0.0  2017-03-30      189.0        190.0\n",
       "2  168551247     ar  11881432.0        0.0  2017-03-30       78.0        149.0\n",
       "3  168549788     ip    295469.0        0.0  2017-03-30       16.0        242.0\n",
       "4  168551248     ip  21393368.0        0.0  2017-03-30       87.0         87.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(song_df_5.take(5), columns=song_df_5.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from previous sanity check  \n",
    "song_df.select('play_time', 'song_length', 'paid_flag').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1  Remove robots on filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_count = song_df_4.select('uid').groupBy('uid').count().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the 95 percentile to be 2965\n",
    "count_ceiling = uid_count.approxQuantile(\"count\", [0.95], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"95 percentile of play counts is {:0}\".format(count_ceiling[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_count.printSchema()\n",
    "valid_uid = uid_count.filter(uid_count['count'] <= count_ceiling[0])\n",
    "# .toPandas() removed,  for join df purpose below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of valid users = {0:0}, \\n number of valid plays = {1:.2e}\"\n",
    "      .format(valid_uid.shape[0], valid_uid[\"count\"].sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ! ! Save valid_uid to a local csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_uid.repartition(1).write.csv('../Data/valid_uid', header=True)\n",
    "# in case session is terminated, to save time, just read from csv to load the DataFrame\n",
    "# active_uid = spark.read.csv('../Data/active_uid.csv')\n",
    "# valid_uid = spark.read.csv('../Data/valid_uid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_id = StructType([StructField('uid', StringType(), False),\n",
    "                        StructField('count', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_uid = spark.read.csv('../Data/valid_uid.csv', schema=schema_id, header=True)\n",
    "valid_uid_2 = valid_uid.select(valid_uid.uid.cast('string').alias('valid_uid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.4891472E7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.6849104E8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.68749728E8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.68148144E8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.4707248E8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      valid_uid\n",
       "0   9.4891472E7\n",
       "1   1.6849104E8\n",
       "2  1.68749728E8\n",
       "3  1.68148144E8\n",
       "4   1.4707248E8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(valid_uid_2.take(5), columns=valid_uid_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uid', 'string'), ('count', 'int')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_uid.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ! ! remain only valid (non-robot) uid   ----->    song_df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df_valid = song_df_5.join(valid_uid, song_df_5.uid == valid_uid.uid, how='inner') \\\n",
    "                         .persist(storageLevel=MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(song_df_valid.take(5), columns=song_df_valid.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Create Churn Label on Distinct Account Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid play log dated from 20170329 to 20170512, \n",
    "# use last 2 week as churn window \n",
    "active_uid = song_df_valid.filter(song_df_valid.date > '2017-04-29') \\\n",
    "                          .select(song_df_valid.uid.alias('active_uid')) \\\n",
    "                          .distinct()\n",
    "\n",
    "active_uid.repartition(1).write.csv('../Data/active_uid', header=True)                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(active_uid,StringType,true)))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_uid = spark.read.csv('../Data/active_uid.csv', header=True)\n",
    "active_uid.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on valid_uid, active_uid are both 1 row for each account\n",
    "uid_label = valid_uid.select('valid_uid') \\\n",
    "                     .join(active_uid, valid_uid.valid_uid == active_uid.active_uid, 'left_outer') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_label = uid_label.withColumn('churn', uid_label.active_uid.isNull().astype(IntegerType())) \\\n",
    "                     .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(valid_uid,StringType,true),StructField(active_uid,StringType,true),StructField(churn,IntegerType,false)))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid_label.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "pd.DataFrame(uid_label.take(5), columns=uid_label.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write output to csv, share with team members\n",
    "uid_label.select('uid', 'churn').repartition(1).write.csv('../Data/uid_label', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Downsample such that churn categories (0, 1) weigh equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|churn|count|\n",
      "+-----+-----+\n",
      "|    1|51541|\n",
      "|    0|83885|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uid_label.groupBy('churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of account is: 135,426\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of account is: {0:,}\".format(51541+83885))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churn vs not churn is 61.442451%\n"
     ]
    }
   ],
   "source": [
    "ratio = 51541/83885\n",
    "print(\"churn vs not churn is {0:%}\".format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_uid = uid_label.drop('active_uid').sampleBy('churn', fractions={1:0.01, 0:0.006}, seed=0)\n",
    "# sampled_uid.groupBy('churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(valid_uid,StringType,true),StructField(churn,IntegerType,false)))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_uid.cache()\n",
    "sampled_uid.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Read logs (Play, Search, Download) that are in the sample selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: float, device: string, song_id: float, song_type: float, date: date, play_time: double, song_length: double, churn: int]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_sample.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_sample = song_df_valid.drop('valid_uid') \\\n",
    "                           .drop('count') \\\n",
    "                           .filter(song_df_valid.date < '2017-04-29') \\\n",
    "                           .filter(song_df_valid.date > '2017-03-29') \\\n",
    "                           .join(sampled_uid, sampled_uid.valid_uid == song_df_valid.uid, 'inner') \\\n",
    "                           .drop(sampled_uid.valid_uid) \\\n",
    "                           .persist(MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386875"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv for future direct load-in\n",
    "play_sample.repartition(1).write.csv('../Data/play_sample', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(uid,StringType,true),StructField(device,StringType,true),StructField(song_id,StringType,true),StructField(song_type,StringType,true),StructField(date,StringType,true),StructField(play_time,StringType,true),StructField(song_length,StringType,true),StructField(churn,StringType,true)))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_sample = spark.read.csv('../Data/play_sample.csv', header=True)\n",
    "play_sample.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>device</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_type</th>\n",
       "      <th>date</th>\n",
       "      <th>play_time</th>\n",
       "      <th>song_length</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.67861376E8</td>\n",
       "      <td>ip</td>\n",
       "      <td>5728245.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.67861376E8</td>\n",
       "      <td>ip</td>\n",
       "      <td>492209.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>127.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.67861376E8</td>\n",
       "      <td>ip</td>\n",
       "      <td>3418611.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.67861376E8</td>\n",
       "      <td>ip</td>\n",
       "      <td>3226009.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.67861376E8</td>\n",
       "      <td>ip</td>\n",
       "      <td>4963794.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.67861376E8</td>\n",
       "      <td>ip</td>\n",
       "      <td>3351562.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.67861376E8</td>\n",
       "      <td>ip</td>\n",
       "      <td>2.2797708E7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.67861376E8</td>\n",
       "      <td>ip</td>\n",
       "      <td>4140214.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.67861376E8</td>\n",
       "      <td>ip</td>\n",
       "      <td>492209.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>2.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.67861376E8</td>\n",
       "      <td>ip</td>\n",
       "      <td>4388183.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid device      song_id song_type        date play_time  \\\n",
       "0  1.67861376E8     ip    5728245.0       0.0  2017-03-31      39.0   \n",
       "1  1.67861376E8     ip     492209.0       0.0  2017-03-31     127.0   \n",
       "2  1.67861376E8     ip    3418611.0       0.0  2017-03-31       1.0   \n",
       "3  1.67861376E8     ip    3226009.0       0.0  2017-03-31       0.0   \n",
       "4  1.67861376E8     ip    4963794.0       0.0  2017-03-31       0.0   \n",
       "5  1.67861376E8     ip    3351562.0       0.0  2017-03-31       0.0   \n",
       "6  1.67861376E8     ip  2.2797708E7       0.0  2017-03-31       0.0   \n",
       "7  1.67861376E8     ip    4140214.0       0.0  2017-03-31      14.0   \n",
       "8  1.67861376E8     ip     492209.0       0.0  2017-03-31       2.0   \n",
       "9  1.67861376E8     ip    4388183.0       0.0  2017-03-31       0.0   \n",
       "\n",
       "  song_length churn  \n",
       "0        39.0     0  \n",
       "1       272.0     0  \n",
       "2       233.0     0  \n",
       "3       257.0     0  \n",
       "4       220.0     0  \n",
       "5       238.0     0  \n",
       "6       229.0     0  \n",
       "7        14.0     0  \n",
       "8       272.0     0  \n",
       "9         3.0     0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(play_sample.take(10), columns=play_sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uid', 'string'),\n",
       " ('device', 'string'),\n",
       " ('song_id', 'string'),\n",
       " ('song_type', 'string'),\n",
       " ('date', 'string'),\n",
       " ('play_time', 'string'),\n",
       " ('song_length', 'string'),\n",
       " ('churn', 'string')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Read Download & Search Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_down = sc.textFile(name=\"../Data/all_download.log.fn\")\n",
    "# schema = ['uid','device','song_id','song_name','singer','paid_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_down.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLineDown(line):\n",
    "    fields = line.split(\"\\t\")\n",
    "    if len(fields) == 7:\n",
    "        try: \n",
    "            uid = str(fields[0])\n",
    "            song_id = str(fields[2])\n",
    "            song_name = str(fields[3])\n",
    "            file_name = str(fields[6])\n",
    "            return Row(uid, song_id, song_name, file_name)\n",
    "        except:\n",
    "            return -1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_down = StructType([StructField('uid', StringType(), False),\n",
    "                          StructField('song_id_down', StringType(), False),\n",
    "                          StructField('song_name_down', StringType(), False),\n",
    "                          StructField('file_name', StringType(), True),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(schema_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse download log \n",
    "down = rdd_down.map(parseLineDown).filter(lambda x: x!=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_df = spark.createDataFrame(down, schema_down)\n",
    "pd.DataFrame(down_df.take(5), columns=down_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_df_2 = down_df.withColumn('date_str', trim(down_df.file_name.substr(1,9))) \\\n",
    "                   .withColumn(\"unix_date\", unix_timestamp('date_str', 'yyyyMMdd')) \\\n",
    "                   .withColumn(\"date_down\", from_unixtime('unix_date').cast(DateType())) \\\n",
    "                   .drop('date_str') \\\n",
    "                   .drop('unix_date') \\\n",
    "                   .drop('file_name') \\\n",
    "                   .dropna(how='any', subset=['song_id_down']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>song_id_down</th>\n",
       "      <th>song_name_down</th>\n",
       "      <th>date_down</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>442554.0</td>\n",
       "      <td>小酒窝</td>\n",
       "      <td>2017-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>6334611.0</td>\n",
       "      <td>社会摇</td>\n",
       "      <td>2017-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>9867382.0</td>\n",
       "      <td>台阶</td>\n",
       "      <td>2017-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>6660691.0</td>\n",
       "      <td>一次就好-(电影《夏洛特烦恼》暖水曲)</td>\n",
       "      <td>2017-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>157606.0</td>\n",
       "      <td>一路上有你</td>\n",
       "      <td>2017-03-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid  song_id_down        song_name_down   date_down\n",
       "0  168019808.0      442554.0                  小酒窝   2017-03-30\n",
       "1  168019808.0     6334611.0                  社会摇   2017-03-30\n",
       "2  168019808.0     9867382.0                   台阶   2017-03-30\n",
       "3  168019808.0     6660691.0  一次就好-(电影《夏洛特烦恼》暖水曲)   2017-03-30\n",
       "4  168019808.0      157606.0                一路上有你   2017-03-30"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(down_df_2.take(5), columns=down_df_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write download log of all dates to csv, for future usage\n",
    "down_df_2.repartition(1).write.csv('../Data/down', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_all = spark.read.csv('../Data/download_all.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+----------+\n",
      "|         uid|song_id_down|      song_name_down| date_down|\n",
      "+------------+------------+--------------------+----------+\n",
      "|1.68019808E8|    442554.0|                 小酒窝|2017-03-30|\n",
      "|1.68019808E8|   6334611.0|                 社会摇|2017-03-30|\n",
      "|1.68019808E8|   9867382.0|                  台阶|2017-03-30|\n",
      "|1.68019808E8|   6660691.0| 一次就好-(电影《夏洛特烦恼》暖水曲)|2017-03-30|\n",
      "|1.68019808E8|    157606.0|               一路上有你|2017-03-30|\n",
      "|1.68019808E8|   3372481.0|        十年 (OT：明年今日)|2017-03-30|\n",
      "|1.68019808E8|   3216525.0|       你不知道的事(Live版)|2017-03-30|\n",
      "|1.68019808E8|   6427523.0|             Victory|2017-03-30|\n",
      "|1.68019808E8|   6538686.0|                  野子|2017-03-30|\n",
      "|1.68019808E8|   9327383.0|なんでもないや-[没什么大不了](...|2017-03-30|\n",
      "|1.68019808E8|   6716077.0|我们不该这样的-(电视剧《北上广不...|2017-03-30|\n",
      "|1.68019808E8|    350052.0|                爱情转移|2017-03-30|\n",
      "|1.68019808E8|   3638735.0|               咱们结婚吧|2017-03-30|\n",
      "|1.68019808E8|    409466.0|        AlwaysOnline|2017-03-30|\n",
      "|1.68019808E8|   1076725.0|Change The World(...|2017-03-30|\n",
      "|1.68019808E8|   7130410.0|  Kyoto(Feat. Sirah)|2017-03-30|\n",
      "|1.68019808E8|    912614.0|               客官不可以|2017-03-30|\n",
      "|1.68019808E8|   3654821.0|                  犯贱|2017-03-30|\n",
      "|  1.680504E8|   6544560.0|Let's Not Fall In...|2017-03-30|\n",
      "|1.68019808E8|   6673573.0|   What Do You Mean？|2017-03-30|\n",
      "+------------+------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "down_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uid', 'string'),\n",
       " ('song_id_down', 'string'),\n",
       " ('song_name_down', 'string'),\n",
       " ('date_down', 'string')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat All Steps Above for Search Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nohup: ignoring input \\t 20170330_1_search.log',\n",
       " '168050208 \\tar \\t2017-03-30 00:00:00 \\t%E5%87%A4%E5%87%B0%E4%BC%A0%E5%A5%87 \\t 20170330_1_search.log',\n",
       " '167659521 \\tar \\t2017-03-30 00:00:01 \\t%E5%B0%8F%E6%B4%B2 \\t 20170330_1_search.log',\n",
       " '168411725 \\tip \\t2017-03-30 00:00:01 \\t%e5%ae%b9%e4%b8%ad%e5%b0%94%e7%94%b2%20%20%e9%ab%98%e5%8e%9f%e7%ba%a2 \\t 20170330_1_search.log',\n",
       " '167852824 \\tar \\t2017-03-30 00:00:02 \\t%E9%83%BD%E6%98%AF%E5%85%84%E5%BC%9F \\t 20170330_1_search.log']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_search = sc.textFile(name=\"../Data/all_search.log.fn\")\n",
    "rdd_search.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLineSearch(line):\n",
    "    fields = line.split(\"\\t\")\n",
    "    if len(fields) == 5:\n",
    "        try:\n",
    "            uid = float(fields[0])\n",
    "            date_str = str(fields[2])\n",
    "            search_query = str(fields[3])\n",
    "            return Row(uid, date_str, search_query)\n",
    "        \n",
    "        except:\n",
    "            return -1\n",
    "    else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_search = StructType([StructField('uid', FloatType(), False),\n",
    "                            StructField('search_date_str', StringType(), False),\n",
    "                            StructField('search_query', StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = rdd_search.map(parseLineSearch).filter(lambda x: x!=-1).filter(lambda x: len(x) == len(schema_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df = spark.createDataFrame(search, schema_search).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>search_date_str</th>\n",
       "      <th>search_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168050208.0</td>\n",
       "      <td>2017-03-30 00:00:00</td>\n",
       "      <td>%E5%87%A4%E5%87%B0%E4%BC%A0%E5%A5%87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167659520.0</td>\n",
       "      <td>2017-03-30 00:00:01</td>\n",
       "      <td>%E5%B0%8F%E6%B4%B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168411728.0</td>\n",
       "      <td>2017-03-30 00:00:01</td>\n",
       "      <td>%e5%ae%b9%e4%b8%ad%e5%b0%94%e7%94%b2%20%20%e9%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167852832.0</td>\n",
       "      <td>2017-03-30 00:00:02</td>\n",
       "      <td>%E9%83%BD%E6%98%AF%E5%85%84%E5%BC%9F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168043776.0</td>\n",
       "      <td>2017-03-30 00:00:02</td>\n",
       "      <td>%E8%91%AB%E8%8A%A6%E4%B8%9D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid       search_date_str  \\\n",
       "0  168050208.0  2017-03-30 00:00:00    \n",
       "1  167659520.0  2017-03-30 00:00:01    \n",
       "2  168411728.0  2017-03-30 00:00:01    \n",
       "3  167852832.0  2017-03-30 00:00:02    \n",
       "4  168043776.0  2017-03-30 00:00:02    \n",
       "\n",
       "                                        search_query  \n",
       "0              %E5%87%A4%E5%87%B0%E4%BC%A0%E5%A5%87   \n",
       "1                                %E5%B0%8F%E6%B4%B2   \n",
       "2  %e5%ae%b9%e4%b8%ad%e5%b0%94%e7%94%b2%20%20%e9%...  \n",
       "3              %E9%83%BD%E6%98%AF%E5%85%84%E5%BC%9F   \n",
       "4                       %E8%91%AB%E8%8A%A6%E4%B8%9D   "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(search_df.take(5), columns=search_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df.repartition(1).write.csv(\"../Data/search_all\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_all = spark.read.csv(\"../Data/search_all.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uid', 'string'), ('search_date_str', 'string'), ('search_query', 'string')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8. Merge search_all and down_all against play_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = play_sample.select('uid').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id.repartition(1).write.csv('../Data/sample_id', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_sample = sample_id.join(down_all, sample_id.uid == down_all.uid, 'inner') \\\n",
    "                       .drop(down_all.uid) \\\n",
    "                       .dropna(how='any', subset=['date_down', 'song_id_down']) \n",
    "        \n",
    "        \n",
    "down_sample.repartition(1).write.csv('../Data/download_sample', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_sample = spark.read.csv('../Data/download_sample.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------------+----------+\n",
      "|        uid|song_id_down| song_name_down| date_down|\n",
      "+-----------+------------+---------------+----------+\n",
      "|1.6804656E8|     93305.0|      真心至上(完整版)|2017-03-30|\n",
      "|1.6804656E8| 2.2830568E7|2017社会鼓 (Remix)|2017-03-30|\n",
      "|1.6804656E8|  2.349608E7|              路|2017-03-30|\n",
      "|1.6804656E8| 2.3037352E7|         现实社会之混|2017-03-30|\n",
      "|1.6804656E8| 2.3288484E7|              肉|2017-03-30|\n",
      "|1.6804656E8| 2.0275342E7|         皮皮虾我们走|2017-03-30|\n",
      "|1.6804656E8| 2.0873728E7|           大话西游|2017-03-30|\n",
      "|1.6804656E8|   6989312.0|       三生三世十里桃花|2017-03-30|\n",
      "|1.6804656E8| 2.0671304E7|             人心|2017-03-30|\n",
      "|1.6804656E8|   6692062.0|        乱世烽火出英雄|2017-03-30|\n",
      "|1.6804656E8|   9932335.0|        何必执着一张脸|2017-03-30|\n",
      "|1.6804656E8|   5376245.0|          起伏的日子|2017-03-30|\n",
      "|1.6804656E8|   9868920.0|        她只喝酒不说话|2017-03-30|\n",
      "|1.6804656E8|   9930485.0|         姑娘 你怂了|2017-03-30|\n",
      "|1.6804656E8|   7105507.0|    你照顾好他.我四海为家|2017-03-30|\n",
      "|1.6804656E8|   9862302.0|         我已收起锋芒|2017-03-30|\n",
      "|1.6804656E8|   9859250.0|        春风十里不及你|2017-03-30|\n",
      "|1.6804656E8| 1.9247962E7|           春风十里|2017-03-30|\n",
      "|1.6804656E8|  1.074068E7|         胸是软绵绵的|2017-03-30|\n",
      "|1.6804656E8|   9932334.0|          我的小可爱|2017-03-30|\n",
      "+-----------+------------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "down_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sample = sample_id.join(search_all, sample_id.uid == search_all.uid, 'inner') \\\n",
    "                         .drop(search_all.uid) \\\n",
    "                         .dropna('any', subset=['uid', 'search_date_str', 'search_query'])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sample.repartition(1).write.csv('../Data/search_sample', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_freq(df, event_type, window, end_date):\n",
    "    df_win = df.filter(date_add(play_sample.date, window) < end_date)\n",
    "    df_feature = df_win.groupBy('uid').agg(count('uid').alias('freq_' + event_type + '_last_' + str(window) + 'days'))\n",
    "    \n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_1 = compute_freq(play_sample, event_type, window, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|         uid|freq_play_last_3days|\n",
      "+------------+--------------------+\n",
      "|1.67861376E8|                 817|\n",
      "|1.68017424E8|                 557|\n",
      "|1.68273328E8|                 236|\n",
      "|1.68466368E8|                 119|\n",
      "|1.68889152E8|                 380|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,3,7,14,30]:\n",
    "    play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = play_sample.filter(date_add(play_sample.date, 3) < end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df.groupBy('uid').agg(count('uid').alias('freq_' + event_type + '_last_' + str(window) + 'days'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|         uid|freq_play_last_3days|\n",
      "+------------+--------------------+\n",
      "|1.67861376E8|                 817|\n",
      "|1.68017424E8|                 557|\n",
      "|1.68273328E8|                 236|\n",
      "|1.68466368E8|                 119|\n",
      "|1.68889152E8|                 380|\n",
      "| 1.6773736E8|                 107|\n",
      "|1.67869824E8|                 820|\n",
      "|1.58661728E8|                   1|\n",
      "| 1.6776896E8|                1804|\n",
      "|1.68072944E8|                 585|\n",
      "+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'\"date\" is not a numeric column. Aggregation function can only be applied on a numeric column.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o914.min.\n: org.apache.spark.sql.AnalysisException: \"date\" is not a numeric column. Aggregation function can only be applied on a numeric column.;\n\tat org.apache.spark.sql.RelationalGroupedDataset$$anonfun$3.apply(RelationalGroupedDataset.scala:101)\n\tat org.apache.spark.sql.RelationalGroupedDataset$$anonfun$3.apply(RelationalGroupedDataset.scala:98)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.RelationalGroupedDataset.aggregateNumericColumns(RelationalGroupedDataset.scala:98)\n\tat org.apache.spark.sql.RelationalGroupedDataset.min(RelationalGroupedDataset.scala:284)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-4aa1fa9f99b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: '\"date\" is not a numeric column. Aggregation function can only be applied on a numeric column.;'"
     ]
    }
   ],
   "source": [
    "df.select(df.date).groupBy().min('date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_sample.select('play_time', 'song_length').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_sample.groupBy('date').count().orderBy('date').show(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(play_sample.take(5), columns = play_sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_sample.crosstab('churn', 'device').show()\n",
    "# !! Finding: device field is not clean, needs trimming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn - 14% use iphone\n",
    "#### not churn - 17% use iphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_set = set(sampled_uid.select('uid'))\n",
    "with open(\"../Data/all_play_log.log.fn\") as infile:\n",
    "    for line in infile:\n",
    "        if line.splint('\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert RDD to DataFrame, from YouTube Video:\n",
    "https://www.youtube.com/watch?v=dzYEWULDIAQ&list=PLE50-dh6JzC5zo2whIGqJ02CIhP3ysQLX&index=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map RDD to a DataFrame\n",
    "# important to filter away lines that do not contain 10 elements\n",
    "# otherwise, get \"Index out of range error\"\n",
    "\n",
    "df = rdd.map(lambda line: line.split(\"\\t\")).filter(lambda line: len(line) == 10)\n",
    "df = (df.map(lambda line: Row(uid = line[0], \n",
    "                             device = line[1], \n",
    "                             song_id = line[2], \n",
    "                             song_type = line[3], \n",
    "                             singer = line[5], \n",
    "                             play_time = line[6], \n",
    "                             song_length = line[7], \n",
    "                             paid_flag = line[8], \n",
    "                             file_name = line[9]))\n",
    "      .toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
