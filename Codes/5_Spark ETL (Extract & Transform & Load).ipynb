{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1. Install PySpark, Java, Hadoop (required to run PySpark)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ZiweiMengyang/Desktop/Python & Machine Learning/Tiger/Capstone/Codes'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is the Python API for Spark\n",
    "# SparkContext is a public class in PySpark. It is the main entry point for Spark functionality in Python\n",
    "# .getOrCreate() is a classmethod that instantiate a SparkContext\n",
    "\n",
    "from pyspark import SparkContext \n",
    "sc = SparkContext.getOrCreate()\n",
    "# i.e. sc is created as a SparkContext class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Next, create pyspark.sql.SparkSession, which is the main entry point for DataFrame and SQL functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Parse log file into RDD then into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textFile() reads a text from HDFS, a local file system, or Hadoop-supported file system,\n",
    "# and return it as an RDD of strings\n",
    "\n",
    "rdd = sc.textFile(name=\"../Data/all_play_log.log.fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164651375"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes 4 minutes to run.... cautious\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a sample log, get a sense how to create a schema for parsing log file into DataFrame\n",
    "# schema_orig = ['uid','device','song_id','song_type','song_name','singer','play_time','song_length','paid_flag']\n",
    "# df = pd.read_csv('../Data/Play/20170331_1_play.log',delimiter='\\t',header=None,index_col=None,names=schema)\n",
    "# df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(\"\\t\")\n",
    "    if len(fields) == 10:\n",
    "        try: \n",
    "            uid = float(fields[0])\n",
    "            device = str(fields[1])\n",
    "            song_id = float(fields[2])\n",
    "            song_type = float(fields[3])\n",
    "            song_name = str(fields[4])\n",
    "            singer = str(fields[5])\n",
    "            play_time = float(fields[6])\n",
    "            song_length = float(fields[7])\n",
    "            paid_flag = float(fields[8])\n",
    "            file_name = str(fields[9])\n",
    "            return Row(uid, device, song_id, song_type, song_name, singer, play_time, song_length, paid_flag, file_name)\n",
    "        except:\n",
    "            return Row(None)\n",
    "    else:\n",
    "        return Row(None)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Create Row entries that specify column name, to prepare the RDD to convert it to a DataFrame\n",
    "# Always important to filter on field length after splitting, to avoid \"index out of range error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide schema in order to create DataFrame\n",
    "# Spark SQL StructType is the data type representing rows. \n",
    "# A StructType object comprises a list of StructField, which represents a field in a StructType\n",
    "# StructField(name of this field, dataType, nullable)\n",
    "\n",
    "\n",
    "schema = StructType([StructField('uid', FloatType(), False),\n",
    "                     StructField('device', StringType(), True),\n",
    "                     StructField('song_id', FloatType(), False),\n",
    "                     StructField('song_type', FloatType(), True),\n",
    "                     StructField('song_name', StringType(), True),\n",
    "                     StructField('singer', StringType(), True),\n",
    "                     StructField('play_time', FloatType(), False),\n",
    "                     StructField('song_length', FloatType(), True),\n",
    "                     StructField('paid_flag', FloatType(), True),\n",
    "                     StructField('file_name', StringType(), True),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = rdd.map(parseLine).filter(lambda x: len(x) == len(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = spark.createDataFrame(songs, schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>device</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_type</th>\n",
       "      <th>song_name</th>\n",
       "      <th>singer</th>\n",
       "      <th>play_time</th>\n",
       "      <th>song_length</th>\n",
       "      <th>paid_flag</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154422688.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>20870992.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>用情</td>\n",
       "      <td>狮子合唱团</td>\n",
       "      <td>22013.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154421904.0</td>\n",
       "      <td>ip</td>\n",
       "      <td>6560858.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>表情不要悲伤</td>\n",
       "      <td>伯贤&amp;D.O.&amp;张艺兴&amp;朴灿烈</td>\n",
       "      <td>96.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154422624.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>3385963.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Baby, Don't Cry(人鱼的眼泪)</td>\n",
       "      <td>EXO</td>\n",
       "      <td>235868.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154410272.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>6777172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3D-环绕音律1(3D Mix)</td>\n",
       "      <td>McTaiM</td>\n",
       "      <td>164.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154407792.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>19472464.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>刚好遇见你</td>\n",
       "      <td>曲肖冰</td>\n",
       "      <td>24.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid device     song_id  song_type                song_name  \\\n",
       "0  154422688.0    ar   20870992.0        1.0                      用情    \n",
       "1  154421904.0    ip    6560858.0        0.0                  表情不要悲伤    \n",
       "2  154422624.0    ar    3385963.0        1.0  Baby, Don't Cry(人鱼的眼泪)    \n",
       "3  154410272.0    ar    6777172.0        0.0        3D-环绕音律1(3D Mix)    \n",
       "4  154407792.0    ar   19472464.0        0.0                   刚好遇见你    \n",
       "\n",
       "             singer  play_time  song_length  paid_flag           file_name  \n",
       "0            狮子合唱团     22013.0        332.0        0.0   20170301_play.log  \n",
       "1  伯贤&D.O.&张艺兴&朴灿烈        96.0        161.0        0.0   20170301_play.log  \n",
       "2              EXO    235868.0        235.0        0.0   20170301_play.log  \n",
       "3           McTaiM       164.0        237.0        0.0   20170301_play.log  \n",
       "4              曲肖冰        24.0        201.0        0.0   20170301_play.log  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(song_df.take(5), columns=song_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Sanity Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|uid         |count  |\n",
      "+------------+-------+\n",
      "|1685126.0   |8123179|\n",
      "|3.7025504E7 |5903384|\n",
      "|751824.0    |4554030|\n",
      "|1791497.0   |3375423|\n",
      "|497685.0    |3031075|\n",
      "|1062806.0   |2354473|\n",
      "|736305.0    |1848836|\n",
      "|0.0         |1201066|\n",
      "|1749320.0   |835075 |\n",
      "|4.6532272E7 |500025 |\n",
      "|1679121.0   |488562 |\n",
      "|2.8638488E7 |469612 |\n",
      "|637650.0    |243074 |\n",
      "|1.5594824E8 |217988 |\n",
      "|533817.0    |173401 |\n",
      "|3.2166204E7 |156591 |\n",
      "|6.4268008E7 |150167 |\n",
      "|2.6036032E7 |114145 |\n",
      "|3.2104144E7 |99175  |\n",
      "|1.67982848E8|82687  |\n",
      "+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_df.groupBy('uid').count().orderBy('count', ascending = False).show(truncate=False)\n",
    "# takes hours to run... DO NOT rerun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------------------+---------+\n",
      "|summary|     play_time|       song_length|paid_flag|\n",
      "+-------+--------------+------------------+---------+\n",
      "|  count|     163014069|         163014069|163014069|\n",
      "|   mean|           NaN|-260.0733923612918|      0.0|\n",
      "| stddev|           NaN|1070712.7989508465|      0.0|\n",
      "|    min|-2.61648826E11|     -2.14748365E9|      0.0|\n",
      "|    max|           NaN|      1.34396621E9|      0.0|\n",
      "+-------+--------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_df.select('play_time', 'song_length', 'paid_flag').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning: play_time>0, song_length < 0.  song_length if NaN, impute using play_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = song_df.filter(song_df.play_time >= 0) \\\n",
    "                 .filter(song_df.song_length > 0) \\\n",
    "                 .dropna(how='any', subset=['play_time']) \\\n",
    "                 .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1373.approxQuantile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 513.0 failed 1 times, most recent failure: Lost task 3.0 in stage 513.0 (TID 19424, localhost, executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:98)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:100)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-8d10c333a1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msong_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"play_time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mapproxQuantile\u001b[0;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0mrelativeError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m         \u001b[0mjaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1520\u001b[0m         \u001b[0mjaq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misStr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1373.approxQuantile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 513.0 failed 1 times, most recent failure: Lost task 3.0 in stage 513.0 (TID 19424, localhost, executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:98)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:100)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "song_df.approxQuantile(\"play_time\", [0.95], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>device</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_type</th>\n",
       "      <th>song_name</th>\n",
       "      <th>singer</th>\n",
       "      <th>play_time</th>\n",
       "      <th>song_length</th>\n",
       "      <th>paid_flag</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154422688.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>20870992.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>用情</td>\n",
       "      <td>狮子合唱团</td>\n",
       "      <td>22013.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154421904.0</td>\n",
       "      <td>ip</td>\n",
       "      <td>6560858.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>表情不要悲伤</td>\n",
       "      <td>伯贤&amp;D.O.&amp;张艺兴&amp;朴灿烈</td>\n",
       "      <td>96.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154422624.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>3385963.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Baby, Don't Cry(人鱼的眼泪)</td>\n",
       "      <td>EXO</td>\n",
       "      <td>235868.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154410272.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>6777172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3D-环绕音律1(3D Mix)</td>\n",
       "      <td>McTaiM</td>\n",
       "      <td>164.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154407792.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>19472464.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>刚好遇见你</td>\n",
       "      <td>曲肖冰</td>\n",
       "      <td>24.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid device     song_id  song_type                song_name  \\\n",
       "0  154422688.0    ar   20870992.0        1.0                      用情    \n",
       "1  154421904.0    ip    6560858.0        0.0                  表情不要悲伤    \n",
       "2  154422624.0    ar    3385963.0        1.0  Baby, Don't Cry(人鱼的眼泪)    \n",
       "3  154410272.0    ar    6777172.0        0.0        3D-环绕音律1(3D Mix)    \n",
       "4  154407792.0    ar   19472464.0        0.0                   刚好遇见你    \n",
       "\n",
       "             singer  play_time  song_length  paid_flag           file_name  \n",
       "0            狮子合唱团     22013.0        332.0        0.0   20170301_play.log  \n",
       "1  伯贤&D.O.&张艺兴&朴灿烈        96.0        161.0        0.0   20170301_play.log  \n",
       "2              EXO    235868.0        235.0        0.0   20170301_play.log  \n",
       "3           McTaiM       164.0        237.0        0.0   20170301_play.log  \n",
       "4              曲肖冰        24.0        201.0        0.0   20170301_play.log  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(song_df.take(5), columns=song_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **  Finding: Obviously some uid are testing accounts (i.e. robots) and should be excluded from the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_count = song_df.groupBy('uid').count().orderBy('count', ascending = False).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the 95 percentile to be 2965\n",
    "count_ceiling = uid_count.approxQuantile(\"count\", [0.95], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 percentile of play counts is 2965.0\n"
     ]
    }
   ],
   "source": [
    "print(\"95 percentile of play counts is {:0}\".format(count_ceiling[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: float (nullable = false)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uid_count.printSchema()\n",
    "valid_uid = uid_count.filter(uid_count['count'] <= count_ceiling[0])\n",
    "# .toPandas() removed,  for join df purpose below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid users = 135543, \n",
      " number of valid plays = 9.84e+07\n"
     ]
    }
   ],
   "source": [
    "print(\"number of valid users = {0:0}, \\n number of valid plays = {1:.2e}\"\n",
    "      .format(valid_uid.shape[0], valid_uid[\"count\"].sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Save valid_uid to a local csv file !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_uid.repartition(1).write.csv('../Data/valid_uid', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|         uid|count|\n",
      "+------------+-----+\n",
      "| 9.4891472E7| 2965|\n",
      "| 1.6849104E8| 2965|\n",
      "|1.68749728E8| 2965|\n",
      "|1.68148144E8| 2965|\n",
      "| 1.4707248E8| 2965|\n",
      "|1.67592272E8| 2965|\n",
      "|1.68414112E8| 2965|\n",
      "|1.68193264E8| 2964|\n",
      "|1.68230784E8| 2964|\n",
      "|1.66444672E8| 2963|\n",
      "| 7.2036656E7| 2963|\n",
      "|1.68756624E8| 2963|\n",
      "|1.67647824E8| 2963|\n",
      "|1.68999216E8| 2963|\n",
      "|1.68460208E8| 2962|\n",
      "|1.67875504E8| 2962|\n",
      "|1.67628128E8| 2962|\n",
      "|1.68355408E8| 2962|\n",
      "|1.67890272E8| 2962|\n",
      "|1.68555648E8| 2962|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_uid.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remain only valid (non-robot) uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df_valid = song_df.join(valid_uid, on='uid', how='inner') \\\n",
    "                        .select('uid', 'device', 'song_id', 'song_type', 'play_time', 'song_length', 'paid_flag', 'file_name') \\\n",
    "                        .cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----------+---------+---------+-----------+---------+------------------+\n",
      "|         uid|device|    song_id|song_type|play_time|song_length|paid_flag|         file_name|\n",
      "+------------+------+-----------+---------+---------+-----------+---------+------------------+\n",
      "|1.54422688E8|   ar |2.0870992E7|      1.0|  22013.0|      332.0|      0.0| 20170301_play.log|\n",
      "|1.54421904E8|   ip |  6560858.0|      0.0|     96.0|      161.0|      0.0| 20170301_play.log|\n",
      "|1.54422624E8|   ar |  3385963.0|      1.0| 235868.0|      235.0|      0.0| 20170301_play.log|\n",
      "|1.54410272E8|   ar |  6777172.0|      0.0|    164.0|      237.0|      0.0| 20170301_play.log|\n",
      "|1.54407792E8|   ar |1.9472464E7|      0.0|     24.0|      201.0|      0.0| 20170301_play.log|\n",
      "|1.54422688E8|   ar |   891952.0|      0.0|    300.0|      300.0|      0.0| 20170301_play.log|\n",
      "|1.54408096E8|   ar |  4623962.0|      0.0|    243.0|      243.0|      0.0| 20170301_play.log|\n",
      "|1.54422576E8|   ar |   703750.0|      0.0|    207.0|      207.0|      0.0| 20170301_play.log|\n",
      "|1.54417312E8|   ar |  6491500.0|      0.0|     56.0|      184.0|      0.0| 20170301_play.log|\n",
      "|1.54421168E8|   ar |  1967689.0|      0.0|    139.0|      275.0|      0.0| 20170301_play.log|\n",
      "|1.54421856E8|   ar |  6126586.0|      0.0|      4.0|       27.0|      0.0| 20170301_play.log|\n",
      "|1.54422656E8|   ar |1.1914644E7|      0.0|    299.0|      300.0|      0.0| 20170301_play.log|\n",
      "|1.54422592E8|   ar |  6468891.0|      0.0|    261.0|      261.0|      0.0| 20170301_play.log|\n",
      "|1.54419568E8|   ar |1.5196649E7|      0.0|      8.0|       65.0|      0.0| 20170301_play.log|\n",
      "|1.54422096E8|   ar |        0.0|      1.0|      7.0|      345.0|      0.0| 20170301_play.log|\n",
      "|1.54422448E8|   ip |  6247282.0|      0.0|      1.0|      277.0|      0.0| 20170301_play.log|\n",
      "|1.54412736E8|   ip |  4357368.0|      0.0|      4.0|      271.0|      0.0| 20170301_play.log|\n",
      "|1.53985856E8|   ar |  6762438.0|      0.0|      1.0|        1.0|      0.0| 20170301_play.log|\n",
      "|1.54408048E8|   ar |  9374861.0|      0.0|    243.0|      244.0|      0.0| 20170301_play.log|\n",
      "|1.54413536E8|   ar |   848918.0|      0.0|      2.0|       46.0|      0.0| 20170301_play.log|\n",
      "+------------+------+-----------+---------+---------+-----------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_df_valid.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. create churn label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df_valid_2 = song_df_valid.withColumn(\"device\", trim(song_df_valid.device)) \\\n",
    "                               .withColumn('date_str', trim(song_df_valid.file_name.substr(1,9))) \\\n",
    "                               .withColumn('date_string', regexp_replace('date_str', '20170339', '20170329')) \\\n",
    "                               .withColumn(\"unix_date\", unix_timestamp('date_string', 'yyyyMMdd')) \\\n",
    "                               .withColumn(\"date\", from_unixtime('unix_date').cast(DateType())) \\\n",
    "                               .drop('date_str') \\\n",
    "                               .drop('date_string') \\\n",
    "                               .drop('unix_date')\n",
    "                            \n",
    "                                \n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----------+---------+---------+-----------+---------+------------------+----------+\n",
      "|         uid|device|    song_id|song_type|play_time|song_length|paid_flag|         file_name|      date|\n",
      "+------------+------+-----------+---------+---------+-----------+---------+------------------+----------+\n",
      "|1.54422688E8|    ar|2.0870992E7|      1.0|  22013.0|      332.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54421904E8|    ip|  6560858.0|      0.0|     96.0|      161.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54422624E8|    ar|  3385963.0|      1.0| 235868.0|      235.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54410272E8|    ar|  6777172.0|      0.0|    164.0|      237.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54407792E8|    ar|1.9472464E7|      0.0|     24.0|      201.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54422688E8|    ar|   891952.0|      0.0|    300.0|      300.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54408096E8|    ar|  4623962.0|      0.0|    243.0|      243.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54422576E8|    ar|   703750.0|      0.0|    207.0|      207.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54417312E8|    ar|  6491500.0|      0.0|     56.0|      184.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54421168E8|    ar|  1967689.0|      0.0|    139.0|      275.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54421856E8|    ar|  6126586.0|      0.0|      4.0|       27.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54422656E8|    ar|1.1914644E7|      0.0|    299.0|      300.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54422592E8|    ar|  6468891.0|      0.0|    261.0|      261.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54419568E8|    ar|1.5196649E7|      0.0|      8.0|       65.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54422096E8|    ar|        0.0|      1.0|      7.0|      345.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54422448E8|    ip|  6247282.0|      0.0|      1.0|      277.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54412736E8|    ip|  4357368.0|      0.0|      4.0|      271.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.53985856E8|    ar|  6762438.0|      0.0|      1.0|        1.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54408048E8|    ar|  9374861.0|      0.0|    243.0|      244.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "|1.54413536E8|    ar|   848918.0|      0.0|      2.0|       46.0|      0.0| 20170301_play.log|2017-03-01|\n",
      "+------------+------+-----------+---------+---------+-----------+---------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_df_valid_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>device</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_type</th>\n",
       "      <th>play_time</th>\n",
       "      <th>song_length</th>\n",
       "      <th>paid_flag</th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154422688.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>20870992.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22013.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154421904.0</td>\n",
       "      <td>ip</td>\n",
       "      <td>6560858.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154422624.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>3385963.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>235868.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154410272.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>6777172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154407792.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>19472464.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170301_play.log</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid device     song_id  song_type  play_time  song_length  \\\n",
       "0  154422688.0     ar  20870992.0        1.0    22013.0        332.0   \n",
       "1  154421904.0     ip   6560858.0        0.0       96.0        161.0   \n",
       "2  154422624.0     ar   3385963.0        1.0   235868.0        235.0   \n",
       "3  154410272.0     ar   6777172.0        0.0      164.0        237.0   \n",
       "4  154407792.0     ar  19472464.0        0.0       24.0        201.0   \n",
       "\n",
       "   paid_flag           file_name        date  \n",
       "0        0.0   20170301_play.log  2017-03-01  \n",
       "1        0.0   20170301_play.log  2017-03-01  \n",
       "2        0.0   20170301_play.log  2017-03-01  \n",
       "3        0.0   20170301_play.log  2017-03-01  \n",
       "4        0.0   20170301_play.log  2017-03-01  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pandas DataFrame to perform quick sanity check \n",
    "pd.DataFrame(song_df_valid_2.take(5), columns=song_df_valid_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play log dated from 20170301 to 20170512, \n",
    "# use last 2 week as churn window \n",
    "active_uid = song_df_valid_2.filter(song_df_valid_2.date >= '2017-04-29') \\\n",
    "                            .select(song_df_valid_2.uid.alias('active_uid')) \\\n",
    "                            .distinct()\n",
    "\n",
    "active_uid.repartition(1).write.csv('../Data/active_uid', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(active_uid,FloatType,false)))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_uid.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case session is terminated, to save time, just read from csv\n",
    "# active_uid = spark.read.csv('../Data/active_uid.csv')\n",
    "# valid_uid = spark.read.csv('../Data/valid_uid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_df.describe().show()\n",
    "# takes forever to run... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_label = valid_uid.join(active_uid, valid_uid.uid == active_uid.active_uid, 'left_outer') \n",
    "uid_label = uid_label.withColumn('churn', uid_label.active_uid.isNull().astype(IntegerType())).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>count</th>\n",
       "      <th>active_uid</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13586118.0</td>\n",
       "      <td>445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16844004.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22030996.0</td>\n",
       "      <td>347</td>\n",
       "      <td>22030996.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23232528.0</td>\n",
       "      <td>181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23885908.0</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid  count  active_uid  churn\n",
       "0  13586118.0    445         NaN      1\n",
       "1  16844004.0      2         NaN      1\n",
       "2  22030996.0    347  22030996.0      0\n",
       "3  23232528.0    181         NaN      1\n",
       "4  23885908.0     45         NaN      1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "pd.DataFrame(uid_label.take(5), columns=uid_label.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write output to csv, share with team members\n",
    "uid_label.select('uid', 'churn').repartition(1).write.csv('../Data/uid_label', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Downsample such that churn categories (0, 1) weigh equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-291ad6b8662c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muid_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'churn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "uid_label.groupBy('churn').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find out churn weight of original data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6144245097454849"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "51541/83885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|churn|count|\n",
      "+-----+-----+\n",
      "|    1| 2610|\n",
      "|    0| 2520|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_uid = uid_label.drop('active_uid', 'count').sampleBy('churn', fractions={1:0.05, 0:0.03}, seed=0)\n",
    "sampled_uid.groupBy('churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: float, churn: int]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_uid.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Read logs (Play, Search, Download) that are in the sample selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_sample = song_df_valid_2.filter(song_df_valid_2.date < 20170429) \\\n",
    "                             .join(sampled_uid, sampled_uid.uid == song_df_valid_2.uid, 'inner') \\\n",
    "                             .drop(sampled_uid.uid) \\\n",
    "                             .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------------+\n",
      "|summary|  play_time|      song_length|\n",
      "+-------+-----------+-----------------+\n",
      "|  count|    2720028|          2720028|\n",
      "|   mean|        NaN|247.7157280182452|\n",
      "| stddev|        NaN|299.5685704649973|\n",
      "|    min|-0.07745105|             -1.0|\n",
      "|    max|        NaN|          32757.0|\n",
      "+-------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_sample.select('play_time', 'song_length').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding: delete play_time = NaN, play_time < 0,  \n",
    "### If song_length < play_time, then impute it to play_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    date| count|\n",
      "+--------+------+\n",
      "|20170424| 24675|\n",
      "|20170414| 37302|\n",
      "|20170428| 42661|\n",
      "|20170411| 42966|\n",
      "|20170308| 47115|\n",
      "|20170426| 47236|\n",
      "|20170309| 47630|\n",
      "|20170423| 48384|\n",
      "|20170425| 49475|\n",
      "|20170420| 49599|\n",
      "|20170422| 50720|\n",
      "|20170307| 51115|\n",
      "|20170427| 51433|\n",
      "|20170421| 51708|\n",
      "|20170418| 52080|\n",
      "|20170419| 52158|\n",
      "|20170417| 53383|\n",
      "|20170306| 54877|\n",
      "|20170339| 55152|\n",
      "|20170413| 59191|\n",
      "|20170415| 60249|\n",
      "|20170403| 61104|\n",
      "|20170416| 62363|\n",
      "|20170412| 62977|\n",
      "|20170305| 63909|\n",
      "|20170410| 66176|\n",
      "|20170408| 69513|\n",
      "|20170409| 72411|\n",
      "|20170407| 72465|\n",
      "|20170304| 72795|\n",
      "|20170405| 72981|\n",
      "|20170406| 73693|\n",
      "|20170303| 76124|\n",
      "|20170404| 94512|\n",
      "|20170302|102326|\n",
      "|20170402|111841|\n",
      "|20170401|124312|\n",
      "|20170330|130265|\n",
      "|20170301|147427|\n",
      "|20170331|153725|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_sample.groupBy('date').count().orderBy('count').show(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    date| count|\n",
      "+--------+------+\n",
      "|20170301|147427|\n",
      "|20170302|102326|\n",
      "|20170303| 76124|\n",
      "|20170304| 72795|\n",
      "|20170305| 63909|\n",
      "|20170306| 54877|\n",
      "|20170307| 51115|\n",
      "|20170308| 47115|\n",
      "|20170309| 47630|\n",
      "|20170330|130265|\n",
      "|20170331|153725|\n",
      "|20170339| 55152|\n",
      "|20170401|124312|\n",
      "|20170402|111841|\n",
      "|20170403| 61104|\n",
      "|20170404| 94512|\n",
      "|20170405| 72981|\n",
      "|20170406| 73693|\n",
      "|20170407| 72465|\n",
      "|20170408| 69513|\n",
      "|20170409| 72411|\n",
      "|20170410| 66176|\n",
      "|20170411| 42966|\n",
      "|20170412| 62977|\n",
      "|20170413| 59191|\n",
      "|20170414| 37302|\n",
      "|20170415| 60249|\n",
      "|20170416| 62363|\n",
      "|20170417| 53383|\n",
      "|20170418| 52080|\n",
      "|20170419| 52158|\n",
      "|20170420| 49599|\n",
      "|20170421| 51708|\n",
      "|20170422| 50720|\n",
      "|20170423| 48384|\n",
      "|20170424| 24675|\n",
      "|20170425| 49475|\n",
      "|20170426| 47236|\n",
      "|20170427| 51433|\n",
      "|20170428| 42661|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_sample.groupBy('date').count().orderBy('date').show(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>device</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_type</th>\n",
       "      <th>play_time</th>\n",
       "      <th>song_length</th>\n",
       "      <th>paid_flag</th>\n",
       "      <th>date</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71357496.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>23498554.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71357496.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>20274308.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71357496.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>10104502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71357496.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>9871706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71357496.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>23498554.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid device     song_id  song_type  play_time  song_length  \\\n",
       "0  71357496.0     ar  23498554.0        0.0        0.0        230.0   \n",
       "1  71357496.0     ar  20274308.0        0.0        0.0        226.0   \n",
       "2  71357496.0     ar  10104502.0        0.0        0.0        291.0   \n",
       "3  71357496.0     ar   9871706.0        0.0        0.0        319.0   \n",
       "4  71357496.0     ar  23498554.0        0.0        0.0        230.0   \n",
       "\n",
       "   paid_flag      date  churn  \n",
       "0        0.0  20170404      1  \n",
       "1        0.0  20170404      1  \n",
       "2        0.0  20170404      1  \n",
       "3        0.0  20170404      1  \n",
       "4        0.0  20170404      1  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(play_sample.take(5), columns = play_sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------+\n",
      "|         uid|max(song_length)|max(play_time)|\n",
      "+------------+----------------+--------------+\n",
      "| 9.0333984E7|             0.0|           0.0|\n",
      "| 1.1485704E8|             0.0|           0.0|\n",
      "| 5.2824928E7|             0.0|           0.0|\n",
      "|  2.347707E7|           466.0|           0.0|\n",
      "| 1.6914496E8|             0.0|           0.0|\n",
      "|1.69146096E8|             0.0|           0.0|\n",
      "|1.65142592E8|             0.0|           0.0|\n",
      "| 1.5493048E8|           299.0|           0.0|\n",
      "|    400904.0|           372.0|           0.0|\n",
      "| 7.8326016E7|             0.0|           0.0|\n",
      "| 1.5685424E8|             0.0|           0.0|\n",
      "|1.55374192E8|          5038.0|           0.0|\n",
      "|1.66336304E8|           327.0|           0.0|\n",
      "|1.11451968E8|           294.0|           0.0|\n",
      "|1.54030576E8|           280.0|           0.0|\n",
      "| 7.1357496E7|           319.0|           0.0|\n",
      "| 1.5377896E8|             0.0|           0.0|\n",
      "| 9.4849952E7|             0.0|           0.0|\n",
      "| 1.6782832E8|            47.0|           0.0|\n",
      "|1.63193792E8|             0.0|           0.0|\n",
      "+------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_sample.groupBy(play_sample.uid) \\\n",
    "           .agg({\"play_time\": \"avg\", \"play_time\": \"min\", \"play_time\": \"max\", \\\n",
    "                 \"song_length\": \"avg\", \"song_length\": \"min\", \"song_length\": \"max\"}) \\\n",
    "           .orderBy('max(play_time)') \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-------------------+--------------+--------------+\n",
      "|churn|max(song_length)|     avg(song_type)|max(play_time)|avg(paid_flag)|\n",
      "+-----+----------------+-------------------+--------------+--------------+\n",
      "|    1|         14724.0|0.09861014618311559|           NaN|           0.0|\n",
      "|    0|         32757.0|0.10539881703994151|  3.39338957E9|           0.0|\n",
      "+-----+----------------+-------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_sample.groupBy(play_sample.churn) \\\n",
    "           .agg({\"play_time\": \"avg\", \"play_time\": \"sum\", \"play_time\": \"min\", \"play_time\": \"max\", \\\n",
    "                 \"song_length\": \"avg\", \"song_length\": \"sum\", \"song_length\": \"min\", \"song_length\": \"max\", \\\n",
    "                 \"song_type\": \"avg\", \\\n",
    "                 \"paid_flag\": \"avg\"}) \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------+\n",
      "|churn_device|     ar|    ip|\n",
      "+------------+-------+------+\n",
      "|           1| 719224|119929|\n",
      "|           0|1554978|325897|\n",
      "+------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_sample.crosstab('churn', 'device').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn - 14% use iphone\n",
    "#### not churn - 17% use iphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  *** Finding: device field is not clean, needs trimming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_set = set(sampled_uid.select('uid'))\n",
    "with open(\"../Data/all_play_log.log.fn\") as infile:\n",
    "    for line in infile:\n",
    "        if line.splint('\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert RDD to DataFrame, from YouTube Video:\n",
    "https://www.youtube.com/watch?v=dzYEWULDIAQ&list=PLE50-dh6JzC5zo2whIGqJ02CIhP3ysQLX&index=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-6a902676b401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m df = (df.map(lambda line: Row(uid = line[0], \n\u001b[0m\u001b[1;32m      7\u001b[0m                              \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                              \u001b[0msong_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \"\"\"\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# map RDD to a DataFrame\n",
    "# important to filter away lines that do not contain 10 elements\n",
    "# otherwise, get \"Index out of range error\"\n",
    "\n",
    "df = rdd.map(lambda line: line.split(\"\\t\")).filter(lambda line: len(line) == 10)\n",
    "df = (df.map(lambda line: Row(uid = line[0], \n",
    "                             device = line[1], \n",
    "                             song_id = line[2], \n",
    "                             song_type = line[3], \n",
    "                             singer = line[5], \n",
    "                             play_time = line[6], \n",
    "                             song_length = line[7], \n",
    "                             paid_flag = line[8], \n",
    "                             file_name = line[9]))\n",
    "      .toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------+---------+--------------------+---------+-----------+--------------------+---------+----------+\n",
      "|device|         file_name|paid_flag|play_time|              singer|  song_id|song_length|           song_name|song_type|       uid|\n",
      "+------+------------------+---------+---------+--------------------+---------+-----------+--------------------+---------+----------+\n",
      "|   ar | 20170301_play.log|       0 |   22013 |              狮子合唱团 |20870993 |       332 |                 用情 |       1 |154422682 |\n",
      "|   ip | 20170301_play.log|       0 |      96 |    伯贤&D.O.&张艺兴&朴灿烈 | 6560858 |       161 |             表情不要悲伤 |       0 |154421907 |\n",
      "|   ar | 20170301_play.log|       0 |  235868 |                EXO | 3385963 |       235 |Baby, Don't Cry(人...|       1 |154422630 |\n",
      "|   ar | 20170301_play.log|       0 |     164 |             McTaiM | 6777172 |       237 |   3D-环绕音律1(3D Mix) |       0 |154410267 |\n",
      "|   ar | 20170301_play.log|       0 |      24 |                曲肖冰 |19472465 |       201 |              刚好遇见你 |       0 |154407793 |\n",
      "|   ar | 20170301_play.log|       0 |  275249 |            SpeXial | 3198036 |         0 |              只唱给你听 |       1 |154422626 |\n",
      "|   ar | 20170301_play.log|       0 |     300 |               筷子兄弟 |  891952 |       300 |   老男孩-(电影《老男孩》主题曲) |       0 |154422681 |\n",
      "|   ar | 20170301_play.log|       0 |     243 |               网络歌手 | 4623962 |       243 |             预谋 许佳慧 |       0 |154408091 |\n",
      "|   ar | 20170301_play.log|       0 |     207 |               儿童歌曲 |  703750 |       207 |            爸爸妈妈听我说 |       0 |154422571 |\n",
      "|   ar | 20170301_play.log|       0 |      56 |        Edward Maya | 6491500 |       184 |        Stereo Love |       0 |154417311 |\n",
      "|   ar | 20170301_play.log|       0 |     139 |                刘德华 | 1967689 |       275 |    悟-(电影《新少林寺》主题曲) |       0 |154421166 |\n",
      "|   ar | 20170301_play.log|       0 |       4 |                 童丽 | 6126586 |        27 |        老情歌(27秒铃声版) |       0 |154421859 |\n",
      "|   ar | 20170301_play.log|       0 |     299 |杨丞琳[一闪一闪亮晶晶的钻石女士]...|11914644 |       300 |                 温柔 |       0 |154422660 |\n",
      "|   ar | 20170301_play.log|       0 |     261 |                薛之谦 | 6468891 |       261 |                 演员 |       0 |154422590 |\n",
      "|   ar | 20170301_play.log|       0 |       8 |               MC马克 |15196649 |        65 |            我太帅了万人爱 |       0 |154419565 |\n",
      "|   ar | 20170301_play.log|       0 |      26 |                林俊杰 | 7143177 |         0 |超越无限-(电影《听·见 林俊杰》...|       0 |154414286 |\n",
      "|   ar | 20170301_play.log|       0 |       7 |听30音乐网首发Qq369849635 |       0 |       345 |           预谋dj 许佳慧 |       1 |154422089 |\n",
      "|   ip | 20170301_play.log|       0 |       1 |                罗百吉 | 6247282 |       277 |            假日(DJ版) |       0 |154422443 |\n",
      "|   ip | 20170301_play.log|       0 |       4 |               慢摇舞曲 | 4357368 |       271 |英文(Dj,2012 超好听的英文...|       0 |154412729 |\n",
      "|   ar | 20170301_play.log|       0 |       1 |               手机铃声 | 6762438 |         1 |         门铃声(1秒铃声版) |       0 |153985859 |\n",
      "+------+------------------+---------+---------+--------------------+---------+-----------+--------------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
